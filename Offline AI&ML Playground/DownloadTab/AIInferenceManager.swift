import Foundation
import SwiftUI
import MLX
import MLXNN
import MLXRandom
import MLXLLM
import MLXLMCommon
import Hub

/// Manager for handling real on-device AI inference using MLX Swift
@MainActor
class AIInferenceManager: ObservableObject {
    @Published var isModelLoaded = false
    @Published var loadingProgress: Float = 0.0
    @Published var loadingStatus = "Ready"
    @Published var lastError: String?
    
    private var currentModel: AIModel?
    private var modelContainer: ModelContainer?
    private var modelConfiguration: ModelConfiguration?
    private var isCurrentlyLoading = false
    
    /// Initialize the inference manager
    init() {
        print("ü§ñ AIInferenceManager initialized")
        setupLogging()
        setupMemoryPressureMonitoring()
    }
    
    /// Setup comprehensive logging
    private func setupLogging() {
        print("üìã Setting up comprehensive logging system")
        print("üì± Device info: \(ProcessInfo.processInfo.machineDescription)")
        print("üíæ Available memory: \(ByteCountFormatter.string(fromByteCount: Int64(ProcessInfo.processInfo.physicalMemory), countStyle: .memory))")
        print("üîß MLX Swift availability: \(isMLXSwiftAvailable ? "‚úÖ Available" : "‚ùå Not Available")")
    }
    
    /// Setup memory pressure monitoring
    private func setupMemoryPressureMonitoring() {
        print("üìä Setting up memory pressure monitoring")
        // Monitor memory usage periodically
        Task {
            while true {
                let _ = getMemoryUsage()
                let memoryPressure = getMemoryPressure()
                
                if memoryPressure > 0.8 && isModelLoaded {
                    print("‚ö†Ô∏è High memory pressure detected (\(memoryPressure)%), considering model unload")
                }
                
                try await Task.sleep(nanoseconds: 30_000_000_000) // Check every 30 seconds
            }
        }
    }
    
    /// Load a specific AI model with proper memory management
    /// - Parameter model: The model to load
    func loadModel(_ model: AIModel) async throws {
        
        // Prevent concurrent model loading
        guard !isCurrentlyLoading else {
            print("‚ö†Ô∏è Model loading already in progress, skipping duplicate request")
            throw AIInferenceError.configurationError("Model loading already in progress")
        }
        
        isCurrentlyLoading = true
        defer { isCurrentlyLoading = false }
        
        print("üöÄ Starting model loading process")
        print("üìã Model: \(model.name) (\(model.id))")
        print("üì¶ Type: \(model.type)")
        print("üíæ Size: \(model.formattedSize)")
        print("üè† Repository: \(model.huggingFaceRepo)")
        print("üìÑ Filename: \(model.filename)")
        
        // **CRITICAL FIX**: Properly handle model switching
        if isModelLoaded {
            print("üîÑ Switching models - unloading current model first")
            await unloadModelAsync()
            
            // Wait for memory to be freed
            await waitForMemoryCleanup()
        }
        
        // Check if model exists locally first
        let localModelPath = getLocalModelPath(for: model)
        let isLocallyAvailable = FileManager.default.fileExists(atPath: localModelPath.path)
        
        print("üìÅ Local model path: \(localModelPath.path)")
        print("üîç Model exists locally: \(isLocallyAvailable)")
        
        await MainActor.run {
            loadingProgress = 0.0
            loadingStatus = "Initializing..."
            lastError = nil
        }
        
        do {
            await MainActor.run {
                loadingProgress = 0.1
                loadingStatus = "Creating model configuration..."
            }
            
            let config = createModelConfiguration(for: model)
            print("‚öôÔ∏è Created model configuration: \(config.id)")
            
            await MainActor.run {
                loadingProgress = 0.2
                loadingStatus = "Preparing model container..."
            }
            
            // Create Hub API for custom download location - only download if not available locally
            let hub = HubApi(downloadBase: getModelDownloadDirectory())
            print("üìÅ Using download directory: \(getModelDownloadDirectory().path)")
            
            // If model exists locally, inform Hub API to use local files
            if isLocallyAvailable {
                print("‚úÖ Using locally cached model files")
                await MainActor.run {
                    loadingProgress = 0.5
                    loadingStatus = "Loading from local cache..."
                }
            } else {
                print("‚¨áÔ∏è Model not found locally, will download from repository")
                await MainActor.run {
                    loadingProgress = 0.3
                    loadingStatus = "Downloading model weights..."
                }
            }
            
            // Load model container with proper error handling
            print("üîÑ Loading model container with configuration: \(config.id)")
            modelContainer = try await LLMModelFactory.shared.loadContainer(
                hub: hub,
                configuration: config
            ) { progress in
                let progressStatus = isLocallyAvailable ? "Loading" : "Downloading"
                let baseProgress = isLocallyAvailable ? 0.5 : 0.3
                let progressRange = isLocallyAvailable ? 0.4 : 0.6
                
                print("üìä Model \(progressStatus.lowercased()) progress: \(progress.fractionCompleted * 100)%")
                Task { @MainActor in
                    self.loadingProgress = Float(baseProgress + (progress.fractionCompleted * progressRange))
                    self.loadingStatus = "\(progressStatus): \(Int(progress.fractionCompleted * 100))%"
                }
            }
            
            await MainActor.run {
                loadingProgress = 0.9
                loadingStatus = "Finalizing model initialization..."
            }
            print("‚úÖ Model container loaded successfully")
            
            // If we downloaded the model, mark it as downloaded
            if !isLocallyAvailable {
                markModelAsDownloaded(model)
            }
            
            // Small delay to ensure everything is properly initialized
            try await Task.sleep(nanoseconds: 500_000_000) // 0.5 seconds
            
            await MainActor.run {
                loadingProgress = 1.0
                loadingStatus = "Model loaded successfully"
                isModelLoaded = true
                currentModel = model
                modelConfiguration = config
            }
            
            print("üéâ Model loaded successfully: \(model.name)")
            print("üîó Source: \(isLocallyAvailable ? "Local Cache" : "Downloaded")")
            print("üìà Final memory usage: \(ByteCountFormatter.string(fromByteCount: Int64(getMemoryUsage()), countStyle: .memory))")
            print("üìä Memory pressure: \(getMemoryPressure())")
            
        } catch {
            await MainActor.run {
                lastError = "Failed to load model: \(error.localizedDescription)"
                loadingStatus = "Error loading model"
                isModelLoaded = false
            }
            print("‚ùå Error loading model: \(error.localizedDescription)")
            print("üîç Error details: \(String(describing: error))")
            
            // Clean up on error
            await cleanupOnError()
            throw error
        }
    }
    
    /// Generate text using the loaded model
    /// - Parameters:
    ///   - prompt: The input prompt
    ///   - maxTokens: Maximum number of tokens to generate
    ///   - temperature: Sampling temperature (0.0 - 1.0)
    /// - Returns: Generated text
    func generateText(
        prompt: String,
        maxTokens: Int = 512,
        temperature: Float = 0.7
    ) async throws -> String {
        
        guard isModelLoaded, let _ = currentModel else {
            print("‚ùå Cannot generate text: Model not loaded")
            throw AIInferenceError.modelNotLoaded
        }
        
        print("üîÆ Starting text generation")
        print("üìù Prompt: \(String(prompt.prefix(100)))")
        print("‚öôÔ∏è Max tokens: \(maxTokens), Temperature: \(temperature)")
        
        do {
            guard let container = modelContainer else {
                print("‚ùå Model container is nil")
                throw AIInferenceError.modelNotLoaded
            }
            
            print("üèÉ‚Äç‚ôÇÔ∏è Performing inference with model container")
            
            let result = try await container.perform { context in
                print("üîß Context ready, preparing input")
                
                // Create user input
                let userInput = UserInput(prompt: prompt)
                print("üì§ Created user input")
                
                // Prepare input using processor
                let input = try await context.processor.prepare(input: userInput)
                print("‚úÖ Input prepared successfully")
                
                // Setup generation parameters
                let parameters = GenerateParameters(
                    maxTokens: maxTokens,
                    temperature: temperature,
                    topP: 0.9
                )
                print("‚öôÔ∏è Generation parameters set: maxTokens=\(parameters.maxTokens), temp=\(String(describing: parameters.temperature)), topP=\(String(describing: parameters.topP))")
                
                // Generate text using MLX
                var generatedText = ""
                let _ = try MLXLMCommon.generate(
                    input: input,
                    parameters: parameters,
                    context: context
                ) { tokens in
                    let text = context.tokenizer.decode(tokens: tokens)
                    generatedText += text
                    print("üîÑ Generated tokens: \(String(text.prefix(50)))")
                    return .more
                }
                
                print("‚úÖ Text generation completed")
                return generatedText
            }
            
            print("üéØ Final generated text length: \(result.count) characters")
            print("üìã Generated text preview: \(String(result.prefix(200)))")
            return result
            
        } catch {
            print("‚ùå Error generating text: \(error.localizedDescription)")
            print("üîç Error details: \(String(describing: error))")
            throw error
        }
    }
    
    /// Generate a streaming response
    /// - Parameters:
    ///   - prompt: The input prompt
    ///   - maxTokens: Maximum number of tokens to generate
    ///   - temperature: Sampling temperature
    /// - Returns: AsyncStream of generated text chunks
    func generateStreamingText(
        prompt: String,
        maxTokens: Int = 512,
        temperature: Float = 0.7
    ) -> AsyncStream<String> {
        
        print("üåä Starting streaming text generation")
        print("üìù Prompt: \(String(prompt.prefix(100)))")
        
        return AsyncStream { continuation in
            Task {
                do {
                    guard isModelLoaded, let _ = currentModel else {
                        print("‚ùå Cannot stream text: Model not loaded")
                        continuation.finish()
                        return
                    }
                    
                    guard let container = modelContainer else {
                        print("‚ùå Model container is nil for streaming")
                        continuation.finish()
                        return
                    }
                    
                    print("üèÉ‚Äç‚ôÇÔ∏è Performing streaming inference")
                    
                    let _ = try await container.perform { context in
                        print("üîß Streaming context ready")
                        
                        // Create user input
                        let userInput = UserInput(prompt: prompt)
                        
                        // Prepare input using processor
                        let input = try await context.processor.prepare(input: userInput)
                        print("‚úÖ Streaming input prepared")
                        
                        // Setup generation parameters
                        let parameters = GenerateParameters(
                            maxTokens: maxTokens,
                            temperature: temperature,
                            topP: 0.9
                        )
                        
                        // Generate text with streaming using MLX
                        var previousLength = 0
                        let _ = try MLXLMCommon.generate(
                            input: input,
                            parameters: parameters,
                            context: context
                        ) { tokens in
                            let fullText = context.tokenizer.decode(tokens: tokens)
                            
                            // Only yield the new part since last time
                            let newText = String(fullText.dropFirst(previousLength))
                            previousLength = fullText.count
                            
                            if !newText.isEmpty {
                                print("üåä Streaming new chunk: \(String(newText.prefix(20)))")
                                continuation.yield(newText)
                            }
                            return .more
                        }
                        
                        return ""
                    }
                    
                    print("‚úÖ Streaming generation completed")
                    continuation.finish()
                    
                } catch {
                    print("‚ùå Error in streaming generation: \(error.localizedDescription)")
                    continuation.finish()
                }
            }
        }
    }
    
    /// Unload the current model (synchronous version)
    func unloadModel() {
        Task {
            await unloadModelAsync()
        }
    }
    
    /// Unload the current model asynchronously with proper cleanup
    func unloadModelAsync() async {
        print("üóëÔ∏è Starting async model unload process")
        let initialMemory = getMemoryUsage()
        
        // Clear model references first
        modelContainer = nil
        modelConfiguration = nil
        currentModel = nil
        
        // Update UI state
        await MainActor.run {
            isModelLoaded = false
            loadingProgress = 0.0
            loadingStatus = "Ready"
            lastError = nil
        }
        
        // Force multiple rounds of garbage collection for MLX
        await performDeepMemoryCleanup()
        
        let finalMemory = getMemoryUsage()
        let memoryFreed = initialMemory > finalMemory ? initialMemory - finalMemory : 0
        
        print("‚úÖ Model unloaded successfully")
        print("üìà Memory freed: \(ByteCountFormatter.string(fromByteCount: Int64(memoryFreed), countStyle: .memory))")
        print("üìà Final memory usage: \(ByteCountFormatter.string(fromByteCount: Int64(finalMemory), countStyle: .memory))")
        print("üìä Memory pressure after cleanup: \(getMemoryPressure())")
    }
    
    /// Perform deep memory cleanup for MLX resources
    private func performDeepMemoryCleanup() async {
        print("üßπ Performing deep memory cleanup")
        
        // Multiple rounds of MLX cleanup
        for i in 1...3 {
            print("üîÑ Cleanup round \(i)/3")
            
            // Force MLX evaluation with empty array to trigger cleanup
            MLX.eval([])
            
            // Give the system time to perform garbage collection
            try? await Task.sleep(nanoseconds: 200_000_000) // 0.2 seconds
            
            // Force Swift garbage collection
            autoreleasepool {
                // Force deallocation of any remaining references
            }
        }
        
        print("‚úÖ Deep memory cleanup completed")
    }
    
    /// Wait for memory to be properly cleaned up
    private func waitForMemoryCleanup() async {
        print("‚è≥ Waiting for memory cleanup to complete")
        let maxWaitTime = 5.0 // Maximum 5 seconds
        let startTime = Date()
        let initialMemory = getMemoryUsage()
        
        while Date().timeIntervalSince(startTime) < maxWaitTime {
            try? await Task.sleep(nanoseconds: 500_000_000) // 0.5 seconds
            
            let currentMemory = getMemoryUsage()
            let memoryReduction = initialMemory > currentMemory ? 
                Double(initialMemory - currentMemory) / Double(initialMemory) : 0
            
            // If memory has been reduced by at least 10%, consider cleanup complete
            if memoryReduction > 0.1 {
                print("‚úÖ Memory cleanup detected (reduction: \(Int(memoryReduction * 100))%)")
                break
            }
        }
        
        print("‚è±Ô∏è Memory cleanup wait completed")
    }
    
    /// Clean up resources on error
    private func cleanupOnError() async {
        print("üßπ Cleaning up resources after error")
        
        modelContainer = nil
        modelConfiguration = nil
        currentModel = nil
        
        await MainActor.run {
            isModelLoaded = false
            loadingProgress = 0.0
        }
        
        // Perform cleanup but don't wait as long
        MLX.eval([])
        try? await Task.sleep(nanoseconds: 100_000_000) // 0.1 seconds
        
        print("‚úÖ Error cleanup completed")
    }
    
    /// Get current memory pressure (0.0 to 1.0)
    private func getMemoryPressure() -> Double {
        let totalMemory = ProcessInfo.processInfo.physicalMemory
        let usedMemory = getMemoryUsage()
        
        let pressure = Double(usedMemory) / Double(totalMemory)
        return min(max(pressure, 0.0), 1.0)
    }
    
    /// Create model configuration for different model types
    private func createModelConfiguration(for model: AIModel) -> ModelConfiguration {
        print("üîß Creating model configuration for: \(model.name)")
        
        // Try to determine the best configuration based on model name
        if model.name.lowercased().contains("llama") {
            if model.name.contains("3.2-1B") {
                print("üìã Using Llama 3.2 1B configuration")
                return ModelConfiguration(
                    id: "mlx-community/Llama-3.2-1B-Instruct-4bit",
                    overrideTokenizer: "PreTrainedTokenizer",
                    defaultPrompt: "Hello! How can I help you today?"
                )
            } else if model.name.contains("3.2-3B") {
                print("üìã Using Llama 3.2 3B configuration")
                return ModelConfiguration(
                    id: "mlx-community/Llama-3.2-3B-Instruct-4bit",
                    overrideTokenizer: "PreTrainedTokenizer",
                    defaultPrompt: "Hello! How can I help you today?"
                )
            } else {
                print("üìã Using default Llama configuration")
                return ModelConfiguration(
                    id: "mlx-community/Llama-3.2-1B-Instruct-4bit",
                    overrideTokenizer: "PreTrainedTokenizer",
                    defaultPrompt: "Hello! How can I help you today?"
                )
            }
        } else if model.name.lowercased().contains("mistral") {
            print("üìã Using Mistral configuration")
            return ModelConfiguration(
                id: "mlx-community/Mistral-7B-Instruct-v0.3-4bit",
                overrideTokenizer: "PreTrainedTokenizer",
                defaultPrompt: "Hello! How can I help you today?"
            )
        } else if model.name.lowercased().contains("phi") {
            print("üìã Using Phi configuration")
            return ModelConfiguration(
                id: "mlx-community/Phi-3.5-mini-instruct-4bit",
                overrideTokenizer: "PreTrainedTokenizer",
                defaultPrompt: "Hello! How can I help you today?"
            )
        } else if model.name.lowercased().contains("code") || model.type == .code {
            if model.name.lowercased().contains("deepseek") {
                print("üìã Using DeepSeek Coder configuration")
                return ModelConfiguration(
                    id: "mlx-community/DeepSeek-Coder-1.3B-Instruct-4bit",
                    overrideTokenizer: "PreTrainedTokenizer",
                    defaultPrompt: "// Write a function to"
                )
            } else if model.name.lowercased().contains("starcoder") {
                print("üìã Using StarCoder configuration")
                return ModelConfiguration(
                    id: "mlx-community/starcoder2-3b-4bit",
                    overrideTokenizer: "PreTrainedTokenizer",
                    defaultPrompt: "# Write a Python function that"
                )
            } else {
                print("üìã Using default Code model configuration")
                return ModelConfiguration(
                    id: "mlx-community/CodeLlama-7B-Instruct-4bit",
                    overrideTokenizer: "PreTrainedTokenizer",
                    defaultPrompt: "// Complete this code:"
                )
            }
        } else {
            print("üìã Using default model configuration")
            return ModelConfiguration(
                id: "mlx-community/Llama-3.2-1B-Instruct-4bit",
                overrideTokenizer: "PreTrainedTokenizer",
                defaultPrompt: "Hello! How can I help you today?"
            )
        }
    }
    
    /// Get model download directory
    private func getModelDownloadDirectory() -> URL {
        let documentsDir = FileManager.default.urls(for: .documentDirectory, in: .userDomainMask).first!
        let modelsDir = documentsDir.appendingPathComponent("MLXModels")
        
        // Create directory if it doesn't exist
        try? FileManager.default.createDirectory(at: modelsDir, withIntermediateDirectories: true)
        print("üìÅ Models directory: \(modelsDir.path)")
        
        return modelsDir
    }
    
    /// Get the local path for a specific model
    private func getLocalModelPath(for model: AIModel) -> URL {
        let modelsDir = getModelDownloadDirectory()
        
        // Create a more comprehensive path structure for the model
        let modelDirectory = modelsDir.appendingPathComponent(model.id, isDirectory: true)
        
        // Check if it's a directory-based model (MLX style) or single file
        if model.filename.hasSuffix(".gguf") || model.filename.hasSuffix(".bin") {
            // Single file model
            return modelsDir.appendingPathComponent("\(model.id)-\(model.filename)")
        } else {
            // Multi-file model - check for standard MLX files
            let configPath = modelDirectory.appendingPathComponent("config.json")
            let modelPath = modelDirectory.appendingPathComponent("model.safetensors")
            let tokenizerPath = modelDirectory.appendingPathComponent("tokenizer.json")
            
            // Return the directory if any of the key files exist
            if FileManager.default.fileExists(atPath: configPath.path) ||
               FileManager.default.fileExists(atPath: modelPath.path) ||
               FileManager.default.fileExists(atPath: tokenizerPath.path) {
                return modelDirectory
            }
            
            // Fallback to single file approach
            return modelsDir.appendingPathComponent(model.filename)
        }
    }
    
    /// Check if a model is downloaded and available locally
    private func isModelDownloadedLocally(_ model: AIModel) -> Bool {
        let localPath = getLocalModelPath(for: model)
        
        var isDirectory: ObjCBool = false
        let exists = FileManager.default.fileExists(atPath: localPath.path, isDirectory: &isDirectory)
        
        if exists {
            if isDirectory.boolValue {
                // Directory-based model - check for essential files
                let modelDir = localPath
                let configExists = FileManager.default.fileExists(atPath: modelDir.appendingPathComponent("config.json").path)
                let modelExists = FileManager.default.fileExists(atPath: modelDir.appendingPathComponent("model.safetensors").path) ||
                                 FileManager.default.fileExists(atPath: modelDir.appendingPathComponent("pytorch_model.bin").path) ||
                                 FileManager.default.fileExists(atPath: modelDir.appendingPathComponent("model.bin").path)
                
                print("üìÅ Directory model check - Config: \(configExists), Model: \(modelExists)")
                return configExists || modelExists
            } else {
                // Single file model
                do {
                    let attributes = try FileManager.default.attributesOfItem(atPath: localPath.path)
                    let fileSize = attributes[.size] as? Int64 ?? 0
                    let expectedSize = model.sizeInBytes
                    
                    // Check if file size is reasonable (at least 50% of expected size to account for compression)
                    let isValidSize = fileSize > expectedSize / 2
                    print("üìÑ Single file model check - Size: \(ByteCountFormatter.string(fromByteCount: Int64(fileSize), countStyle: .file)) vs Expected: \(ByteCountFormatter.string(fromByteCount: Int64(expectedSize), countStyle: .file)), Valid: \(isValidSize)")
                    return isValidSize
                } catch {
                    print("‚ùå Error checking file size: \(error)")
                    return false
                }
            }
        }
        
        return false
    }
    
    /// Mark a model as downloaded
    private func markModelAsDownloaded(_ model: AIModel) {
        print("‚úÖ Marking model as downloaded: \(model.id)")
        // Since we use file system as source of truth, we don't need to maintain a separate list
        // The isModelDownloadedLocally method will check the actual files
    }
    
    /// Clean up any incomplete or corrupted model downloads
    func cleanupIncompleteDownloads() {
        let modelsDir = getModelDownloadDirectory()
        
        do {
            let contents = try FileManager.default.contentsOfDirectory(at: modelsDir, includingPropertiesForKeys: [.fileSizeKey, .creationDateKey])
            
            for fileURL in contents {
                let attributes = try fileURL.resourceValues(forKeys: [.fileSizeKey, .creationDateKey])
                let fileSize = attributes.fileSize ?? 0
                let creationDate = attributes.creationDate ?? Date()
                
                // Remove files that are very small (likely incomplete downloads) and old
                let isOldFile = Date().timeIntervalSince(creationDate) > 24 * 60 * 60 // Older than 24 hours
                let isTooSmall = fileSize < 1024 * 1024 // Smaller than 1MB
                
                if isOldFile && isTooSmall {
                    print("üßπ Removing incomplete download: \(fileURL.lastPathComponent)")
                    try? FileManager.default.removeItem(at: fileURL)
                }
            }
        } catch {
            print("‚ö†Ô∏è Error during cleanup: \(error)")
        }
    }
    
    /// Get current memory usage
    private func getMemoryUsage() -> UInt64 {
        var info = mach_task_basic_info()
        var count = mach_msg_type_number_t(MemoryLayout<mach_task_basic_info>.size)/4
        
        let kerr: kern_return_t = withUnsafeMutablePointer(to: &info) {
            $0.withMemoryRebound(to: integer_t.self, capacity: 1) {
                task_info(mach_task_self_,
                         task_flavor_t(MACH_TASK_BASIC_INFO),
                         $0,
                         &count)
            }
        }
        
        if kerr == KERN_SUCCESS {
            return info.resident_size
        } else {
            return 0
        }
    }
    
    /// Generate model-specific response (fallback implementation)
    private func generateModelSpecificResponse(
        prompt: String,
        model: AIModel,
        temperature: Float
    ) async throws -> String {
        
        print("üîÑ Using fallback model-specific response generation")
        
        // Simulate processing time
        try await Task.sleep(nanoseconds: UInt64(500_000_000 + Double.random(in: 0...1_000_000_000)))
        
        let modelInfo = "[\(model.name) - MLX Swift Integration Active]"
        
        switch model.type {
        case .llama:
            return """
            \(modelInfo)
            
            I'm a Llama model running on-device with MLX Swift. Your prompt was: "\(prompt)"
            
            MLX Swift Integration Status: ‚úÖ Active
            ‚Ä¢ On-device inference with Apple Silicon optimization
            ‚Ä¢ Memory-efficient model loading
            ‚Ä¢ Hardware-accelerated computation
            ‚Ä¢ Complete privacy with no data leaving your device
            
            Temperature: \(temperature)
            Model: \(model.name)
            Type: Large Language Model
            """
            
        case .mistral:
            return """
            \(modelInfo)
            
            Bonjour! I'm a Mistral model running with MLX Swift. Your query: "\(prompt)"
            
            Mistral AI + MLX Swift Features:
            ‚Ä¢ High-quality instruction following
            ‚Ä¢ Efficient 7B parameter architecture
            ‚Ä¢ Optimized for mobile deployment
            ‚Ä¢ Advanced reasoning capabilities
            ‚Ä¢ On-device privacy and speed
            
            Temperature: \(temperature)
            Model: \(model.name)
            Type: Mistral Language Model
            """
            
        case .code:
            return """
            \(modelInfo)
            
            // Code generation response for: "\(prompt)"
            
            func mlxSwiftInference() {
                // MLX Swift integration is now active!
                print("Loading model with MLX Swift...")
                
                // Features available:
                // - Real code generation with MLX
                // - Syntax highlighting support
                // - Multi-language support
                // - On-device processing
            }
            
            // Temperature: \(temperature)
            // Model: \(model.name)
            """
            
        case .stable_diffusion:
            return """
            \(modelInfo)
            
            üé® Stable Diffusion response for: "\(prompt)"
            
            MLX Swift Image Generation:
            ‚Ä¢ Text-to-image generation
            ‚Ä¢ Style transfer
            ‚Ä¢ Image editing
            ‚Ä¢ Creative AI art
            
            Note: Image generation with MLX Swift ready for implementation.
            
            Temperature: \(temperature)
            Model: \(model.name)
            """
            
        case .general:
            return """
            \(modelInfo)
            
            Hello! I'm a general-purpose AI model running with MLX Swift. You asked: "\(prompt)"
            
            MLX Swift Capabilities:
            ‚Ä¢ General conversation
            ‚Ä¢ Question answering
            ‚Ä¢ Text analysis
            ‚Ä¢ Creative writing
            ‚Ä¢ Problem solving
            
            Running natively on Apple Silicon with MLX Swift optimization.
            
            Temperature: \(temperature)
            Model: \(model.name)
            """
        }
    }
}

/// Errors that can occur during AI inference
enum AIInferenceError: LocalizedError {
    case modelNotLoaded
    case modelFileNotFound
    case invalidModelFormat
    case inferenceError(String)
    case tokenizationError
    case outOfMemory
    case configurationError(String)
    
    var errorDescription: String? {
        switch self {
        case .modelNotLoaded:
            return "No model is currently loaded"
        case .modelFileNotFound:
            return "Model file not found"
        case .invalidModelFormat:
            return "Invalid model file format"
        case .inferenceError(let message):
            return "Inference error: \(message)"
        case .tokenizationError:
            return "Error tokenizing input"
        case .outOfMemory:
            return "Insufficient memory to load model"
        case .configurationError(let message):
            return "Configuration error: \(message)"
        }
    }
}

// MARK: - MLX Swift Integration Extensions

extension AIInferenceManager {
    
    /// Check if MLX Swift is available
    var isMLXSwiftAvailable: Bool {
        print("üîç Checking MLX Swift availability")
        // Check if we can access MLX types
        let available = true // MLX should be available if we got this far
        print("‚úÖ MLX Swift availability: \(available ? "Available" : "Not Available")")
        return available
    }
    
    /// Get system capabilities for running models
    func getSystemCapabilities() -> SystemCapabilities {
        print("üîç Getting system capabilities")
        let processInfo = ProcessInfo.processInfo
        let physicalMemory = processInfo.physicalMemory
        
        let capabilities = SystemCapabilities(
            totalMemory: physicalMemory,
            availableMemory: physicalMemory / 2, // Rough estimate
            hasNeuralEngine: true, // Assume true for Apple Silicon
            hasGPU: true,
            isAppleSilicon: true
        )
        
        print("üíæ Total memory: \(ByteCountFormatter.string(fromByteCount: Int64(capabilities.totalMemory), countStyle: .memory))")
        print("üîì Available memory: \(ByteCountFormatter.string(fromByteCount: Int64(capabilities.availableMemory), countStyle: .memory))")
        print("üß† Neural Engine: \(capabilities.hasNeuralEngine ? "Available" : "Not Available")")
        print("üéÆ GPU: \(capabilities.hasGPU ? "Available" : "Not Available")")
        print("üçé Apple Silicon: \(capabilities.isAppleSilicon ? "Yes" : "No")")
        
        return capabilities
    }
    
    /// Recommend optimal settings for a model
    func getOptimalSettings(for model: AIModel) -> InferenceSettings {
        print("‚öôÔ∏è Getting optimal settings for model: \(model.name)")
        let capabilities = getSystemCapabilities()
        
        // Calculate optimal settings based on model size and system capabilities
        let recommendedBatchSize = capabilities.availableMemory > 16_000_000_000 ? 8 : 4
        let recommendedMaxTokens = model.name.contains("large") ? 256 : 512
        
        let settings = InferenceSettings(
            batchSize: recommendedBatchSize,
            maxTokens: recommendedMaxTokens,
            temperature: 0.7,
            topP: 0.9,
            useGPUAcceleration: true,
            useNeuralEngine: true
        )
        
        print("üìä Recommended batch size: \(settings.batchSize)")
        print("üî¢ Recommended max tokens: \(settings.maxTokens)")
        print("üå°Ô∏è Default temperature: \(settings.temperature)")
        print("üéØ Default top-p: \(settings.topP)")
        
        return settings
    }
}

// MARK: - Supporting Types

struct SystemCapabilities {
    let totalMemory: UInt64
    let availableMemory: UInt64
    let hasNeuralEngine: Bool
    let hasGPU: Bool
    let isAppleSilicon: Bool
}

struct InferenceSettings {
    let batchSize: Int
    let maxTokens: Int
    let temperature: Float
    let topP: Float
    let useGPUAcceleration: Bool
    let useNeuralEngine: Bool
}

// MARK: - ProcessInfo Extension for Machine Description
extension ProcessInfo {
    var machineDescription: String {
        var size = 0
        sysctlbyname("hw.model", nil, &size, nil, 0)
        var machine = [CChar](repeating: 0, count: size)
        sysctlbyname("hw.model", &machine, &size, nil, 0)
        return String(cString: machine)
    }
} 
